<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<h1 id="text-normalization-challenge---english-language-kaggle">Text Normalization Challenge - English Language [Kaggle]</h1>
<h2 id="project-presentation">Project presentation</h2>
<p><a href="https://drive.google.com/open?id=1XvsLropEW1TX9BlitafjH7bBYsBF6O3spm7-Bz0G9XQ" class="uri">https://drive.google.com/open?id=1XvsLropEW1TX9BlitafjH7bBYsBF6O3spm7-Bz0G9XQ</a></p>
<h2 id="jupyter-notebook">Jupyter Notebook</h2>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">
<span class="ex">jupyter</span> notebook text_norm_challenge.ipynb
</code></pre></div>
<h2 id="data-analysis-with-pandas">Data analysis with pandas</h2>
<pre class="shell"><code>
python analysis/dataAnalysis.py

</code></pre>
<h2 id="running">Running</h2>
<h3 id="data-preprocessing-and-tokenization">Data Preprocessing and Tokenization</h3>
<ol>
<li><p>From words to sentences</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
<span class="bu">print</span> (<span class="st">&quot;Hello from data preprocess&quot;</span>)

<span class="im">import</span> csv

db_file <span class="op">=</span> <span class="st">&#39;data/kaggle_norm_competition/en_train.csv&#39;</span>                    <span class="co"># Here you should put the path to the file you want to change</span>

outputFile <span class="op">=</span> <span class="bu">open</span> (<span class="st">&quot;outputTrainDST&quot;</span>, <span class="st">&#39;a&#39;</span>)
<span class="cf">with</span> <span class="bu">open</span> (db_file, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> f:
    reader <span class="op">=</span> csv.reader(f)

    counter <span class="op">=</span> <span class="dv">0</span>
    pastPhrase <span class="op">=</span> <span class="dv">0</span>
    reader.<span class="fu">__next__</span>()
    phraseString <span class="op">=</span> <span class="st">&#39;&#39;</span>
    <span class="cf">for</span> row <span class="kw">in</span> reader:
        <span class="cf">if</span> pastPhrase <span class="op">==</span> <span class="bu">int</span> (row[<span class="dv">0</span>]):
            <span class="co">#We are still on the current phrase</span>
            phraseString <span class="op">+=</span> row[<span class="dv">4</span>] <span class="op">+</span> <span class="st">&#39; &#39;</span>
        <span class="cf">else</span>:
            <span class="bu">print</span> (phraseString, <span class="bu">file</span> <span class="op">=</span> outputFile)
            phraseString <span class="op">=</span> <span class="st">&quot;&quot;</span>
            phraseString <span class="op">+=</span> row[<span class="dv">4</span>] <span class="op">+</span> <span class="st">&#39; &#39;</span>

            counter <span class="op">+=</span> <span class="dv">1</span>
            <span class="cf">if</span> counter <span class="op">==</span> <span class="dv">20000</span>:
                <span class="cf">break</span>

        pastPhrase <span class="op">=</span> <span class="bu">int</span> (row[<span class="dv">0</span>])


    <span class="bu">print</span> (counter)
</code></pre></div></li>
<li><p>Phrases tokenization</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">
<span class="kw">for</span> <span class="ex">l</span> in en de<span class="kw">;</span> <span class="kw">do</span> <span class="kw">for</span> <span class="ex">f</span> in data/kaggle_norm_competition/*.<span class="va">$l</span><span class="kw">;</span> <span class="kw">do</span> <span class="kw">if [[</span> <span class="st">&quot;</span><span class="va">$f</span><span class="st">&quot;</span> <span class="ot">!=</span> *<span class="st">&quot;test&quot;</span>*<span class="kw"> ]]</span>; <span class="kw">then</span> <span class="fu">sed</span> -i <span class="st">&quot;$ d&quot;</span> <span class="va">$f</span><span class="kw">;</span> <span class="kw">fi</span>;  <span class="kw">done</span>; <span class="kw">done</span>
<span class="kw">for</span> <span class="ex">l</span> in en de<span class="kw">;</span> <span class="kw">do</span> <span class="kw">for</span> <span class="ex">f</span> in data/kaggle_norm_competition/*.<span class="va">$l</span><span class="kw">;</span> <span class="kw">do</span> <span class="fu">perl</span> tokenizer.perl -a -no-escape -l <span class="va">$l</span> -q  <span class="op">&lt;</span> <span class="va">$f</span> <span class="op">&gt;</span> <span class="va">$f</span>.atok<span class="kw">;</span> <span class="kw">done</span>; <span class="kw">done</span>

<span class="ex">python</span> preprocess.py -train_src data/kaggle_norm_competition/linesTrainSRC -train_tgt data/kaggle_norm_competition/linesTrainDST -valid_src data/kaggle_norm_competition/linesValSRC -valid_tgt data/kaggle_norm_competition/linesValDST -save_data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt
</code></pre></div></li>
</ol>
<h3 id="training">Training</h3>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">
<span class="ex">python</span> train.py -data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -save_model trained -save_mode best -proj_share_weight
</code></pre></div>
<h3 id="testing">Testing</h3>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">
<span class="ex">python</span> translate.py -model trained.chkpt -vocab data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -src data/kaggle_norm_competition/linesTest
</code></pre></div>
<h2 id="the-code">The code</h2>
<p>After working with the classic Transformer we decided to try some changes in order to classify the words (an info that we already have in the database) and try if this info could help us.</p>
<p>In order to do that, our idea is to add a new Transformer decoder that is trained with the word class.</p>
<h3 id="word-classification-trial">Word classification trial</h3>
<p>The new transformer model will be:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
<span class="kw">class</span> MyTransformer(nn.Module):                                                                                                                                                                                                               
<span class="co">&#39;&#39;&#39; A sequence to sequence model with attention mechanism. &#39;&#39;&#39;</span>                                                                                                                                                                            

<span class="kw">def</span> <span class="fu">__init__</span>(                                                                                                                                                                                                                             
        <span class="va">self</span>, n_src_vocab, n_tgt_vocab, n_cls_vocab, n_max_seq, n_layers<span class="op">=</span><span class="dv">6</span>, n_head<span class="op">=</span><span class="dv">8</span>,                                                                                                                                                     
        d_word_vec<span class="op">=</span><span class="dv">512</span>, d_model<span class="op">=</span><span class="dv">512</span>, d_inner_hid<span class="op">=</span><span class="dv">1024</span>, d_k<span class="op">=</span><span class="dv">64</span>, d_v<span class="op">=</span><span class="dv">64</span>,                                                                                                                                                                    
        dropout<span class="op">=</span><span class="fl">0.1</span>, proj_share_weight<span class="op">=</span><span class="va">True</span>, embs_share_weight<span class="op">=</span><span class="va">True</span>):                                                                                                                                                                     

    <span class="bu">super</span>(MyTransformer, <span class="va">self</span>).<span class="fu">__init__</span>()                                                                                                                                                                                                 
    <span class="va">self</span>.encoder <span class="op">=</span> Encoder(                                                                                                                                                                                                               
        n_src_vocab, n_max_seq, n_layers<span class="op">=</span>n_layers, n_head<span class="op">=</span>n_head,                                                                                                                                                                         
        d_word_vec<span class="op">=</span>d_word_vec, d_model<span class="op">=</span>d_model,                                                                                                                                                                                           
        d_inner_hid<span class="op">=</span>d_inner_hid, dropout<span class="op">=</span>dropout)                                                                                                                                                                                         
    <span class="va">self</span>.decoder <span class="op">=</span> Decoder(                                                                                                                                                                                                               
        n_tgt_vocab, n_max_seq, n_layers<span class="op">=</span>n_layers, n_head<span class="op">=</span>n_head,                                                                                                                                                                         
        d_word_vec<span class="op">=</span>d_word_vec, d_model<span class="op">=</span>d_model,                                                                                                                                                                                           
        d_inner_hid<span class="op">=</span>d_inner_hid, dropout<span class="op">=</span>dropout)                                                                                                                                                                                         
    <span class="va">self</span>.tgt_word_proj <span class="op">=</span> Linear(d_model, n_tgt_vocab, bias<span class="op">=</span><span class="va">False</span>)                                                                                                                                                                         
    <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)                                                                                                                                                                                                    

    <span class="va">self</span>.decoder_cls <span class="op">=</span> Decoder(                                                                                                                                                                                                           
        n_cls_vocab, n_max_seq, n_layers<span class="op">=</span>n_layers, n_head<span class="op">=</span>n_head,                                                                                                                                                                         
        d_word_vec<span class="op">=</span>d_word_vec, d_model<span class="op">=</span>d_model,                                                                                                                                                                                           
        d_inner_hid<span class="op">=</span>d_inner_hid, dropout<span class="op">=</span>dropout)                                                                                                                                                                                         
    <span class="va">self</span>.cls_word_proj <span class="op">=</span> Linear(d_model, n_cls_vocab, bias<span class="op">=</span><span class="va">False</span>)                                                                                                                                                                         

    <span class="cf">assert</span> d_model <span class="op">==</span> d_word_vec, <span class="op">\</span>                                                                                                                                                                                                       
    <span class="co">&#39;To facilitate the residual connections, \                                                                                                                                                                                            </span>
<span class="co">     the dimensions of all module output shall be the same.&#39;</span>                                                                                                                                                                              

    <span class="cf">if</span> proj_share_weight:                                                                                                                                                                                                                 
        <span class="co"># Share the weight matrix between tgt word embedding/projection                                                                                                                                                                   </span>
        <span class="cf">assert</span> d_model <span class="op">==</span> d_word_vec                                                                                                                                                                                                      
        <span class="va">self</span>.tgt_word_proj.weight <span class="op">=</span> <span class="va">self</span>.decoder.tgt_word_emb.weight                                                                                                                                                                      

    <span class="cf">if</span> embs_share_weight:                                                                                                                                                                                                                 
        <span class="co"># Share the weight matrix between src/tgt word embeddings                                                                                                                                                                         </span>
        <span class="co"># assume the src/tgt word vec size are the same                                                                                                                                                                                   </span>
        <span class="cf">assert</span> n_src_vocab <span class="op">==</span> n_tgt_vocab, <span class="op">\</span>                                                                                                                                                                                              
        <span class="co">&quot;To share word embedding table, the vocabulary size of src/tgt shall be the same.&quot;</span>                                                                                                                                                
        <span class="va">self</span>.encoder.src_word_emb.weight <span class="op">=</span> <span class="va">self</span>.decoder.tgt_word_emb.weight                                                                                                                                                               

<span class="kw">def</span> get_trainable_parameters(<span class="va">self</span>):                                                                                                                                                                                                       
    <span class="co">&#39;&#39;&#39; Avoid updating the position encoding &#39;&#39;&#39;</span>                                                                                                                                                                                          
    enc_freezed_param_ids <span class="op">=</span> <span class="bu">set</span>(<span class="bu">map</span>(<span class="bu">id</span>, <span class="va">self</span>.encoder.position_enc.parameters()))                                                                                                                                                          
    dec_freezed_param_ids <span class="op">=</span> <span class="bu">set</span>(<span class="bu">map</span>(<span class="bu">id</span>, <span class="va">self</span>.decoder.position_enc.parameters()))                                                                                                                                                          
    dec_freezed_param_ids_cls <span class="op">=</span> <span class="bu">set</span>(<span class="bu">map</span>(<span class="bu">id</span>, <span class="va">self</span>.decoder_cls.position_enc.parameters()))                                                                                                                                                  

    freezed_param_ids <span class="op">=</span> enc_freezed_param_ids <span class="op">|</span> dec_freezed_param_ids <span class="op">|</span> dec_freezed_param_ids_cls                                                                                                                                         
    <span class="cf">return</span> (p <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters() <span class="cf">if</span> <span class="bu">id</span>(p) <span class="kw">not</span> <span class="kw">in</span> freezed_param_ids)                                                                                                                                                               

<span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, cls):                                                                                                                                                                                                         
    src_seq, src_pos <span class="op">=</span> src                                                                                                                                                                                                                
    tgt_seq, tgt_pos <span class="op">=</span> tgt                                                                                                                                                                                                                
    cls_seq, cls_pos <span class="op">=</span> cls

    <span class="co">#TODO: I don&#39;t understand what this is doing (only will modify the lenght of one sentence, the largest one)                                                                                                                           </span>
    tgt_seq <span class="op">=</span> tgt_seq[:, :<span class="op">-</span><span class="dv">1</span>]                                                                                                                                                                                                             
    tgt_pos <span class="op">=</span> tgt_pos[:, :<span class="op">-</span><span class="dv">1</span>]                                                                                                                                                                                                             

    cls_seq <span class="op">=</span> cls_seq[:, :<span class="op">-</span><span class="dv">1</span>]                                                                                                                                                                                                             
    cls_pos <span class="op">=</span> cls_pos[:, :<span class="op">-</span><span class="dv">1</span>]                                                                                                                                                                                                             

    enc_output, <span class="op">*</span>_ <span class="op">=</span> <span class="va">self</span>.encoder(src_seq, src_pos)                                                                                                                                                                                       

    dec_output, <span class="op">*</span>_ <span class="op">=</span> <span class="va">self</span>.decoder(tgt_seq, tgt_pos, src_seq, enc_output)                                                                                                                                                                  
    seq_logit <span class="op">=</span> <span class="va">self</span>.tgt_word_proj(dec_output)                                                                                                                                                                                            

    dec_cls_output, <span class="op">*</span>_ <span class="op">=</span> <span class="va">self</span>.decoder_cls(cls_seq, cls_pos, src_seq, enc_output)                                                                                                                                                          
    seq_cls_logit <span class="op">=</span> <span class="va">self</span>.tgt_word_proj(dec_cls_output)                                                                                                                                                                                    

    <span class="cf">return</span> seq_logit.view(<span class="op">-</span><span class="dv">1</span>, seq_logit.size(<span class="dv">2</span>)), seq_cls_logit.view(<span class="op">-</span><span class="dv">1</span>, seq_cls_logit.size(<span class="dv">2</span>))
</code></pre></div>
<p>And the loss should be now</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
<span class="kw">def</span> train_epoch(model, training_data, crit, optimizer):                                                                                                                                                                                       
<span class="co">&#39;&#39;&#39; Epoch operation in training phase&#39;&#39;&#39;</span>                                                                                                                                                                                                  

model.train()                                                                                                                                                                                                                             

total_loss <span class="op">=</span> <span class="dv">0</span>                                                                                                                                                                                                                            
n_total_words <span class="op">=</span> <span class="dv">0</span>                                                                                                                                                                                                                         
n_total_correct <span class="op">=</span> <span class="dv">0</span>                                                                                                                                                                                                                       

<span class="cf">for</span> batch <span class="kw">in</span> tqdm(                                                                                                                                                                                                                        
        training_data, mininterval<span class="op">=</span><span class="dv">2</span>,                                                                                                                                                                                                     
        desc<span class="op">=</span><span class="st">&#39;  - (Training)   &#39;</span>, leave<span class="op">=</span><span class="va">False</span>):                                                                                                                                                                                           

    <span class="co"># prepare data                                                                                                                                                                                                                        </span>
    src, tgt, cls <span class="op">=</span> batch                                                                                                                                                                                                                 

    gold <span class="op">=</span> tgt[<span class="dv">0</span>][:, <span class="dv">1</span>:]                                                                                                                                                                                                                  
    class_gold <span class="op">=</span> cls[<span class="dv">0</span>][:,<span class="dv">1</span>:]                                                                                                                                                                                                             

    <span class="co"># forward                                                                                                                                                                                                                             </span>
    optimizer.zero_grad()                                                                                                                                                                                                                 
    pred <span class="op">=</span> model(src, tgt)                                                                                                                                                                                                                

    <span class="co"># backward                                                                                                                                                                                                                            </span>
    loss, n_correct <span class="op">=</span> get_performance(crit, pred, gold)                                                                                                                                                                                   
    loss.backward()                                                                                                                                                                                                                       

    <span class="co"># update parameters                                                                                                                                                                                                                   </span>
    optimizer.step()                                                                                                                                                                                                                      
    optimizer.update_learning_rate()                                                                                                                                                                                                      

    <span class="co"># note keeping                                                                                                                                                                                                                        </span>
    n_words <span class="op">=</span> gold.data.ne(Constants.PAD).<span class="bu">sum</span>()                                                                                                                                                                                           
    n_total_words <span class="op">+=</span> n_words                                                                                                                                                                                                              
    n_total_correct <span class="op">+=</span> n_correct                                                                                                                                                                                                          
    total_loss <span class="op">+=</span> loss.data[<span class="dv">0</span>]                                                                                                                                                                                                            

<span class="cf">return</span> total_loss<span class="op">/</span>n_total_words, n_total_correct<span class="op">/</span>n_total_words
</code></pre></div>
</body>
</html>

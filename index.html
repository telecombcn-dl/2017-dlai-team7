<!--<?xml version="1.0" encoding="utf-8"?>-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">-->
<head>
<!-- 2017-12-12 Tue 15:26 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Text Normalization Challenge</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Daniel Moreno Manzano" />

</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgbd7c562">1. Text Normalization Challenge - English Language [Kaggle]</a>
<ul>
<li><a href="#orgbfab7de">1.1. Jupyter Notebook</a></li>
<li><a href="#org43a0b0c">1.2. Data analysis with pandas</a></li>
<li><a href="#orgd374fc6">1.3. Running</a>
<ul>
<li><a href="#orgbc0d8d5">1.3.1. Data Preprocessing and Tokenization</a></li>
<li><a href="#org7c6fba0">1.3.2. Training</a></li>
<li><a href="#orga58d0c7">1.3.3. Testing</a></li>
</ul>
</li>
<li><a href="#org6e2434b">1.4. The code</a>
<ul>
<li><a href="#orgf611fde">1.4.1. Word classification trial</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgbd7c562" class="outline-2">
<h2 id="orgbd7c562"><span class="section-number-2">1</span> Text Normalization Challenge - English Language [Kaggle]</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgbfab7de" class="outline-3">
<h3 id="orgbfab7de"><span class="section-number-3">1.1</span> Jupyter Notebook</h3>
<div class="outline-text-3" id="text-1-1">
<div class="org-src-container">
<pre class="src src-bash">jupyter notebook text_norm_challenge.ipynb
</pre>
</div>
</div>
</div>

<div id="outline-container-org43a0b0c" class="outline-3">
<h3 id="org43a0b0c"><span class="section-number-3">1.2</span> Data analysis with pandas</h3>
<div class="outline-text-3" id="text-1-2">
<div class="org-src-container">
<pre class="src src-shell">python analysis/dataAnalysis.py
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd374fc6" class="outline-3">
<h3 id="orgd374fc6"><span class="section-number-3">1.3</span> Running</h3>
<div class="outline-text-3" id="text-1-3">
</div><div id="outline-container-orgbc0d8d5" class="outline-4">
<h4 id="orgbc0d8d5"><span class="section-number-4">1.3.1</span> Data Preprocessing and Tokenization</h4>
<div class="outline-text-4" id="text-1-3-1">
</div><ol class="org-ol"><li><a id="org4dcd998"></a>From words to sentences<br /><div class="outline-text-5" id="text-1-3-1-1">
<div class="org-src-container">
<pre class="src src-python">print ("Hello from data preprocess")

import csv

db_file = 'data/kaggle_norm_competition/en_train.csv'                    # Here you should put the path to the file you want to change

outputFile = open ("outputTrainDST", 'a')
with open (db_file, 'r') as f:
    reader = csv.reader(f)

    counter = 0
    pastPhrase = 0
    reader.__next__()
    phraseString = ''
    for row in reader:
	if pastPhrase == int (row[0]):
	    #We are still on the current phrase
	    phraseString += row[4] + ' '
	else:
	    print (phraseString, file = outputFile)
	    phraseString = ""
	    phraseString += row[4] + ' '

	    counter += 1
	    if counter == 20000:
		break

	pastPhrase = int (row[0])


    print (counter)
</pre>
</div>
</div></li>


<li><a id="orgf0778f0"></a>Phrases tokenization<br /><div class="outline-text-5" id="text-1-3-1-2">
<div class="org-src-container">
<pre class="src src-bash">for l in en de; do for f in data/kaggle_norm_competition/*.$l; do if [[ "$f" != *"test"* ]]; then sed -i "$ d" $f; fi;  done; done
for l in en de; do for f in data/kaggle_norm_competition/*.$l; do perl tokenizer.perl -a -no-escape -l $l -q  &lt; $f &gt; $f.atok; done; done

python preprocess.py -train_src data/kaggle_norm_competition/linesTrainSRC -train_tgt data/kaggle_norm_competition/linesTrainDST -valid_src data/kaggle_norm_competition/linesValSRC -valid_tgt data/kaggle_norm_competition/linesValDST -save_data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt
</pre>
</div>
</div></li></ol>
</div>

<div id="outline-container-org7c6fba0" class="outline-4">
<h4 id="org7c6fba0"><span class="section-number-4">1.3.2</span> Training</h4>
<div class="outline-text-4" id="text-1-3-2">
<div class="org-src-container">
<pre class="src src-bash">python train.py -data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -save_model trained -save_mode best -proj_share_weight
</pre>
</div>
</div>
</div>

<div id="outline-container-orga58d0c7" class="outline-4">
<h4 id="orga58d0c7"><span class="section-number-4">1.3.3</span> Testing</h4>
<div class="outline-text-4" id="text-1-3-3">
<div class="org-src-container">
<pre class="src src-bash">python translate.py -model trained.chkpt -vocab data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -src data/kaggle_norm_competition/linesTest
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org6e2434b" class="outline-3">
<h3 id="org6e2434b"><span class="section-number-3">1.4</span> The code</h3>
<div class="outline-text-3" id="text-1-4">
<p>
After working with the classic Transformer we decided to try some changes in order to classify the words (an info that we already have in the database) and try if this info could help us.
</p>

<p>
In order to do that, our idea is to add a new Transformer decoder that is trained with the word class.
</p>
</div>

<div id="outline-container-orgf611fde" class="outline-4">
<h4 id="orgf611fde"><span class="section-number-4">1.4.1</span> Word classification trial</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
The new transformer model will be:
</p>

<div class="org-src-container">
<pre class="src src-python">class MyTransformer(nn.Module):                                                                                                                                                                                                               
''' A sequence to sequence model with attention mechanism. '''                                                                                                                                                                            

def __init__(                                                                                                                                                                                                                             
	self, n_src_vocab, n_tgt_vocab, n_cls_vocab, n_max_seq, n_layers=6, n_head=8,                                                                                                                                                     
	d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,                                                                                                                                                                    
	dropout=0.1, proj_share_weight=True, embs_share_weight=True):                                                                                                                                                                     

    super(MyTransformer, self).__init__()                                                                                                                                                                                                 
    self.encoder = Encoder(                                                                                                                                                                                                               
	n_src_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         
	d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           
	d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         
    self.decoder = Decoder(                                                                                                                                                                                                               
	n_tgt_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         
	d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           
	d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         
    self.tgt_word_proj = Linear(d_model, n_tgt_vocab, bias=False)                                                                                                                                                                         
    self.dropout = nn.Dropout(dropout)                                                                                                                                                                                                    

    self.decoder_cls = Decoder(                                                                                                                                                                                                           
	n_cls_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         
	d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           
	d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         
    self.cls_word_proj = Linear(d_model, n_cls_vocab, bias=False)                                                                                                                                                                         

    assert d_model == d_word_vec, \                                                                                                                                                                                                       
    'To facilitate the residual connections, \                                                                                                                                                                                            
     the dimensions of all module output shall be the same.'                                                                                                                                                                              

    if proj_share_weight:                                                                                                                                                                                                                 
	# Share the weight matrix between tgt word embedding/projection                                                                                                                                                                   
	assert d_model == d_word_vec                                                                                                                                                                                                      
	self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight                                                                                                                                                                      

    if embs_share_weight:                                                                                                                                                                                                                 
	# Share the weight matrix between src/tgt word embeddings                                                                                                                                                                         
	# assume the src/tgt word vec size are the same                                                                                                                                                                                   
	assert n_src_vocab == n_tgt_vocab, \                                                                                                                                                                                              
	"To share word embedding table, the vocabulary size of src/tgt shall be the same."                                                                                                                                                
	self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight                                                                                                                                                               

def get_trainable_parameters(self):                                                                                                                                                                                                       
    ''' Avoid updating the position encoding '''                                                                                                                                                                                          
    enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))                                                                                                                                                          
    dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))                                                                                                                                                          
    dec_freezed_param_ids_cls = set(map(id, self.decoder_cls.position_enc.parameters()))                                                                                                                                                  

    freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids | dec_freezed_param_ids_cls                                                                                                                                         
    return (p for p in self.parameters() if id(p) not in freezed_param_ids)                                                                                                                                                               

def forward(self, src, tgt, cls):                                                                                                                                                                                                         
    src_seq, src_pos = src                                                                                                                                                                                                                
    tgt_seq, tgt_pos = tgt                                                                                                                                                                                                                
    cls_seq, cls_pos = cls

    #TODO: I don't understand what this is doing (only will modify the lenght of one sentence, the largest one)                                                                                                                           
    tgt_seq = tgt_seq[:, :-1]                                                                                                                                                                                                             
    tgt_pos = tgt_pos[:, :-1]                                                                                                                                                                                                             

    cls_seq = cls_seq[:, :-1]                                                                                                                                                                                                             
    cls_pos = cls_pos[:, :-1]                                                                                                                                                                                                             

    enc_output, *_ = self.encoder(src_seq, src_pos)                                                                                                                                                                                       

    dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)                                                                                                                                                                  
    seq_logit = self.tgt_word_proj(dec_output)                                                                                                                                                                                            

    dec_cls_output, *_ = self.decoder_cls(cls_seq, cls_pos, src_seq, enc_output)                                                                                                                                                          
    seq_cls_logit = self.tgt_word_proj(dec_cls_output)                                                                                                                                                                                    

    return seq_logit.view(-1, seq_logit.size(2)), seq_cls_logit.view(-1, seq_cls_logit.size(2))
</pre>
</div>

<p>
And the loss should be now
</p>

<div class="org-src-container">
<pre class="src src-python">def train_epoch(model, training_data, crit, optimizer):                                                                                                                                                                                       
''' Epoch operation in training phase'''                                                                                                                                                                                                  

model.train()                                                                                                                                                                                                                             

total_loss = 0                                                                                                                                                                                                                            
n_total_words = 0                                                                                                                                                                                                                         
n_total_correct = 0                                                                                                                                                                                                                       

for batch in tqdm(                                                                                                                                                                                                                        
	training_data, mininterval=2,                                                                                                                                                                                                     
	desc='  - (Training)   ', leave=False):                                                                                                                                                                                           

    # prepare data                                                                                                                                                                                                                        
    src, tgt, cls = batch                                                                                                                                                                                                                 

    gold = tgt[0][:, 1:]                                                                                                                                                                                                                  
    class_gold = cls[0][:,1:]                                                                                                                                                                                                             

    # forward                                                                                                                                                                                                                             
    optimizer.zero_grad()                                                                                                                                                                                                                 
    pred = model(src, tgt)                                                                                                                                                                                                                

    # backward                                                                                                                                                                                                                            
    loss, n_correct = get_performance(crit, pred, gold)                                                                                                                                                                                   
    loss.backward()                                                                                                                                                                                                                       

    # update parameters                                                                                                                                                                                                                   
    optimizer.step()                                                                                                                                                                                                                      
    optimizer.update_learning_rate()                                                                                                                                                                                                      

    # note keeping                                                                                                                                                                                                                        
    n_words = gold.data.ne(Constants.PAD).sum()                                                                                                                                                                                           
    n_total_words += n_words                                                                                                                                                                                                              
    n_total_correct += n_correct                                                                                                                                                                                                          
    total_loss += loss.data[0]                                                                                                                                                                                                            

return total_loss/n_total_words, n_total_correct/n_total_words
</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Daniel Moreno Manzano</p>
<p class="date">Created: 2017-12-12 Tue 15:26</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>

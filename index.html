<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-12-12 Tue 15:26 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Daniel Moreno Manzano" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgbd7c562">1. Text Normalization Challenge - English Language [Kaggle]</a>
<ul>
<li><a href="#orgbfab7de">1.1. Jupyter Notebook</a></li>
<li><a href="#org43a0b0c">1.2. Data analysis with pandas</a></li>
<li><a href="#orgd374fc6">1.3. Running</a>
<ul>
<li><a href="#orgbc0d8d5">1.3.1. Data Preprocessing and Tokenization</a></li>
<li><a href="#org7c6fba0">1.3.2. Training</a></li>
<li><a href="#orga58d0c7">1.3.3. Testing</a></li>
</ul>
</li>
<li><a href="#org6e2434b">1.4. The code</a>
<ul>
<li><a href="#orgf611fde">1.4.1. Word classification trial</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgbd7c562" class="outline-2">
<h2 id="orgbd7c562"><span class="section-number-2">1</span> Text Normalization Challenge - English Language [Kaggle]</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgbfab7de" class="outline-3">
<h3 id="orgbfab7de"><span class="section-number-3">1.1</span> Jupyter Notebook</h3>
<div class="outline-text-3" id="text-1-1">
<div class="org-src-container">
<pre class="src src-bash">jupyter notebook text_norm_challenge.ipynb
</pre>
</div>
</div>
</div>

<div id="outline-container-org43a0b0c" class="outline-3">
<h3 id="org43a0b0c"><span class="section-number-3">1.2</span> Data analysis with pandas</h3>
<div class="outline-text-3" id="text-1-2">
<div class="org-src-container">
<pre class="src src-shell">python analysis/dataAnalysis.py
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd374fc6" class="outline-3">
<h3 id="orgd374fc6"><span class="section-number-3">1.3</span> Running</h3>
<div class="outline-text-3" id="text-1-3">
</div><div id="outline-container-orgbc0d8d5" class="outline-4">
<h4 id="orgbc0d8d5"><span class="section-number-4">1.3.1</span> Data Preprocessing and Tokenization</h4>
<div class="outline-text-4" id="text-1-3-1">
</div><ol class="org-ol"><li><a id="org4dcd998"></a>From words to sentences<br /><div class="outline-text-5" id="text-1-3-1-1">
<div class="org-src-container">
<pre class="src src-python">print ("Hello from data preprocess")

import csv

db_file = 'data/kaggle_norm_competition/en_train.csv'                    # Here you should put the path to the file you want to change

outputFile = open ("outputTrainDST", 'a')
with open (db_file, 'r') as f:
    reader = csv.reader(f)

    counter = 0
    pastPhrase = 0
    reader.__next__()
    phraseString = ''
    for row in reader:
	if pastPhrase == int (row[0]):
	    #We are still on the current phrase
	    phraseString += row[4] + ' '
	else:
	    print (phraseString, file = outputFile)
	    phraseString = ""
	    phraseString += row[4] + ' '

	    counter += 1
	    if counter == 20000:
		break

	pastPhrase = int (row[0])


    print (counter)
</pre>
</div>
</div></li>


<li><a id="orgf0778f0"></a>Phrases tokenization<br /><div class="outline-text-5" id="text-1-3-1-2">
<div class="org-src-container">
<pre class="src src-bash">for l in en de; do for f in data/kaggle_norm_competition/*.$l; do if [[ "$f" != *"test"* ]]; then sed -i "$ d" $f; fi;  done; done
for l in en de; do for f in data/kaggle_norm_competition/*.$l; do perl tokenizer.perl -a -no-escape -l $l -q  &lt; $f &gt; $f.atok; done; done

python preprocess.py -train_src data/kaggle_norm_competition/linesTrainSRC -train_tgt data/kaggle_norm_competition/linesTrainDST -valid_src data/kaggle_norm_competition/linesValSRC -valid_tgt data/kaggle_norm_competition/linesValDST -save_data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt
</pre>
</div>
</div></li></ol>
</div>

<div id="outline-container-org7c6fba0" class="outline-4">
<h4 id="org7c6fba0"><span class="section-number-4">1.3.2</span> Training</h4>
<div class="outline-text-4" id="text-1-3-2">
<div class="org-src-container">
<pre class="src src-bash">python train.py -data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -save_model trained -save_mode best -proj_share_weight
</pre>
</div>
</div>
</div>

<div id="outline-container-orga58d0c7" class="outline-4">
<h4 id="orga58d0c7"><span class="section-number-4">1.3.3</span> Testing</h4>
<div class="outline-text-4" id="text-1-3-3">
<div class="org-src-container">
<pre class="src src-bash">python translate.py -model trained.chkpt -vocab data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -src data/kaggle_norm_competition/linesTest
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org6e2434b" class="outline-3">
<h3 id="org6e2434b"><span class="section-number-3">1.4</span> The code</h3>
<div class="outline-text-3" id="text-1-4">
<p>
After working with the classic Transformer we decided to try some changes in order to classify the words (an info that we already have in the database) and try if this info could help us.
</p>

<p>
In order to do that, our idea is to add a new Transformer decoder that is trained with the word class.
</p>
</div>

<div id="outline-container-orgf611fde" class="outline-4">
<h4 id="orgf611fde"><span class="section-number-4">1.4.1</span> Word classification trial</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
The new transformer model will be:
</p>

<div class="org-src-container">
<pre class="src src-python">class MyTransformer(nn.Module):                                                                                                                                                                                                               
''' A sequence to sequence model with attention mechanism. '''                                                                                                                                                                            

def __init__(                                                                                                                                                                                                                             
	self, n_src_vocab, n_tgt_vocab, n_cls_vocab, n_max_seq, n_layers=6, n_head=8,                                                                                                                                                     
	d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,                                                                                                                                                                    
	dropout=0.1, proj_share_weight=True, embs_share_weight=True):                                                                                                                                                                     

    super(MyTransformer, self).__init__()                                                                                                                                                                                                 
    self.encoder = Encoder(                                                                                                                                                                                                               
	n_src_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         
	d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           
	d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         
    self.decoder = Decoder(                                                                                                                                                                                                               
	n_tgt_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         
	d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           
	d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         
    self.tgt_word_proj = Linear(d_model, n_tgt_vocab, bias=False)                                                                                                                                                                         
    self.dropout = nn.Dropout(dropout)                                                                                                                                                                                                    

    self.decoder_cls = Decoder(                                                                                                                                                                                                           
	n_cls_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         
	d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           
	d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         
    self.cls_word_proj = Linear(d_model, n_cls_vocab, bias=False)                                                                                                                                                                         

    assert d_model == d_word_vec, \                                                                                                                                                                                                       
    'To facilitate the residual connections, \                                                                                                                                                                                            
     the dimensions of all module output shall be the same.'                                                                                                                                                                              

    if proj_share_weight:                                                                                                                                                                                                                 
	# Share the weight matrix between tgt word embedding/projection                                                                                                                                                                   
	assert d_model == d_word_vec                                                                                                                                                                                                      
	self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight                                                                                                                                                                      

    if embs_share_weight:                                                                                                                                                                                                                 
	# Share the weight matrix between src/tgt word embeddings                                                                                                                                                                         
	# assume the src/tgt word vec size are the same                                                                                                                                                                                   
	assert n_src_vocab == n_tgt_vocab, \                                                                                                                                                                                              
	"To share word embedding table, the vocabulary size of src/tgt shall be the same."                                                                                                                                                
	self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight                                                                                                                                                               

def get_trainable_parameters(self):                                                                                                                                                                                                       
    ''' Avoid updating the position encoding '''                                                                                                                                                                                          
    enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))                                                                                                                                                          
    dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))                                                                                                                                                          
    dec_freezed_param_ids_cls = set(map(id, self.decoder_cls.position_enc.parameters()))                                                                                                                                                  

    freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids | dec_freezed_param_ids_cls                                                                                                                                         
    return (p for p in self.parameters() if id(p) not in freezed_param_ids)                                                                                                                                                               

def forward(self, src, tgt, cls):                                                                                                                                                                                                         
    src_seq, src_pos = src                                                                                                                                                                                                                
    tgt_seq, tgt_pos = tgt                                                                                                                                                                                                                
    cls_seq, cls_pos = cls

    #TODO: I don't understand what this is doing (only will modify the lenght of one sentence, the largest one)                                                                                                                           
    tgt_seq = tgt_seq[:, :-1]                                                                                                                                                                                                             
    tgt_pos = tgt_pos[:, :-1]                                                                                                                                                                                                             

    cls_seq = cls_seq[:, :-1]                                                                                                                                                                                                             
    cls_pos = cls_pos[:, :-1]                                                                                                                                                                                                             

    enc_output, *_ = self.encoder(src_seq, src_pos)                                                                                                                                                                                       

    dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)                                                                                                                                                                  
    seq_logit = self.tgt_word_proj(dec_output)                                                                                                                                                                                            

    dec_cls_output, *_ = self.decoder_cls(cls_seq, cls_pos, src_seq, enc_output)                                                                                                                                                          
    seq_cls_logit = self.tgt_word_proj(dec_cls_output)                                                                                                                                                                                    

    return seq_logit.view(-1, seq_logit.size(2)), seq_cls_logit.view(-1, seq_cls_logit.size(2))
</pre>
</div>

<p>
And the loss should be now
</p>

<div class="org-src-container">
<pre class="src src-python">def train_epoch(model, training_data, crit, optimizer):                                                                                                                                                                                       
''' Epoch operation in training phase'''                                                                                                                                                                                                  

model.train()                                                                                                                                                                                                                             

total_loss = 0                                                                                                                                                                                                                            
n_total_words = 0                                                                                                                                                                                                                         
n_total_correct = 0                                                                                                                                                                                                                       

for batch in tqdm(                                                                                                                                                                                                                        
	training_data, mininterval=2,                                                                                                                                                                                                     
	desc='  - (Training)   ', leave=False):                                                                                                                                                                                           

    # prepare data                                                                                                                                                                                                                        
    src, tgt, cls = batch                                                                                                                                                                                                                 

    gold = tgt[0][:, 1:]                                                                                                                                                                                                                  
    class_gold = cls[0][:,1:]                                                                                                                                                                                                             

    # forward                                                                                                                                                                                                                             
    optimizer.zero_grad()                                                                                                                                                                                                                 
    pred = model(src, tgt)                                                                                                                                                                                                                

    # backward                                                                                                                                                                                                                            
    loss, n_correct = get_performance(crit, pred, gold)                                                                                                                                                                                   
    loss.backward()                                                                                                                                                                                                                       

    # update parameters                                                                                                                                                                                                                   
    optimizer.step()                                                                                                                                                                                                                      
    optimizer.update_learning_rate()                                                                                                                                                                                                      

    # note keeping                                                                                                                                                                                                                        
    n_words = gold.data.ne(Constants.PAD).sum()                                                                                                                                                                                           
    n_total_words += n_words                                                                                                                                                                                                              
    n_total_correct += n_correct                                                                                                                                                                                                          
    total_loss += loss.data[0]                                                                                                                                                                                                            

return total_loss/n_total_words, n_total_correct/n_total_words
</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Daniel Moreno Manzano</p>
<p class="date">Created: 2017-12-12 Tue 15:26</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>

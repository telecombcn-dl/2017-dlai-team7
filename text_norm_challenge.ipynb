{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization Challenge [Kaggle]\n",
    "\n",
    "AUTHORS: Carlos Arenas, Marc Combalia, Daniel Moreno, Guillem Lahuerta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis with pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from data analysis main2\n",
      "     sentence_id  token_id     class before   after\n",
      "162           14         5  VERBATIM      -       -\n",
      "279           24         3  VERBATIM      #  number\n",
      "724           54        11  VERBATIM      &     and\n",
      "980           75         2  VERBATIM      サ       サ\n",
      "981           75         3  VERBATIM      イ       イ\n",
      "         sentence_id  token_id     class before after  beforeCount  afterCount\n",
      "8762635       662504         3  VERBATIM    - B   - B            2           2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_hist (num_values, name):\n",
    "    plt.clf()\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.bar(range(len(num_values)), list(num_values.values()), align='center')\n",
    "    plt.xticks(range(len(num_values)), list(num_values.keys()))\n",
    "\n",
    "    plt.savefig(name + '.png')\n",
    "\n",
    "def main():\n",
    "    print (\"Hello from data analysis main\")\n",
    "\n",
    "    df = pd.read_csv('data/kaggle_norm_competition/en_train.csv')\n",
    "\n",
    "    columns = df['class'].value_counts()\n",
    "\n",
    "\n",
    "    num_values = {}\n",
    "    percentage_modif = {}\n",
    "\n",
    "    for column in columns.keys():\n",
    "        df_aux = df.loc[df['class'] == column]\n",
    "        df_dif = df_aux.loc[df_aux['before'] != df_aux['after']]\n",
    "\n",
    "        num_values[column] = len(df_aux)\n",
    "        percentage_modif[column] = len (df_dif) / len(df_aux)\n",
    "\n",
    "\n",
    "    make_hist(num_values, 'hist')\n",
    "    make_hist(percentage_modif, 'hist_modif')\n",
    "\n",
    "\n",
    "def main4():\n",
    "    print(\"Hello from data analysis main\")\n",
    "\n",
    "    df = pd.read_csv('data/kaggle_norm_competition/en_train.csv')\n",
    "\n",
    "    columns = df['class'].value_counts()\n",
    "\n",
    "    num_values = {}\n",
    "    percentage_modif = {}\n",
    "\n",
    "    for column in columns.keys():\n",
    "        df_aux = df.loc[df['class'] == column]\n",
    "        df_dif = df_aux.loc[df_aux['before'] != df_aux['after']]\n",
    "\n",
    "        num_values[column] = len(df_aux)\n",
    "        percentage_modif[column] = len(df_dif) / len(df_aux)\n",
    "\n",
    "    make_hist(num_values, 'hist')\n",
    "    make_hist(percentage_modif, 'hist_modif')\n",
    "\n",
    "def main2 ():\n",
    "    print (\"Hello from data analysis main2\")\n",
    "\n",
    "    df = pd.read_csv('data/kaggle_norm_competition/en_train.csv')\n",
    "\n",
    "    df = df.loc[df['class'] == 'VERBATIM']\n",
    "\n",
    "    print (df.head())\n",
    "\n",
    "    #import sys\n",
    "    #sys.exit()\n",
    "\n",
    "    df['before'] = df ['before'].apply (str)\n",
    "    df['beforeCount'] = df ['before'].apply (lambda x: len (x.split(' ')))\n",
    "\n",
    "    df['after'] = df['after'].apply(str)\n",
    "    df['afterCount'] = df ['after'].apply (lambda x: len (x.split(' ')))\n",
    "\n",
    "    columns = df['beforeCount'].value_counts()\n",
    "\n",
    "    print (df.loc[df['beforeCount'] == 2].head())\n",
    "\n",
    "    columns.plot.bar (figsize = (20, 20))\n",
    "\n",
    "    plt.savefig (\"hist_lenPLAIN_DST.png\")\n",
    "def main3():\n",
    "    dictionary = {1: 8957502, 2: 134575, 3: 177305, 4: 68841, 5: 30799, 6: 28000, 7: 23914, 8: 9018, 9: 1567, 10: 921, 11: 1100, 12: 544, 13: 1864, 14: 487, 15: 323, 16: 338, 17: 816, 18: 241, 19: 224, 20: 175, 21: 92, 22: 76, 23: 85, 24: 64, 25: 59, 26: 31, 27: 28, 28: 22, 29: 17, 30: 15, 31: 15, 32: 21, 33: 19, 34: 12, 35: 12, 36: 10, 37: 5, 38: 6, 39: 7, 40: 18, 41: 7, 42: 12, 43: 6, 44: 4, 45: 4, 46: 16, 47: 7, 48: 15, 49: 6, 50: 8, 51: 6, 52: 11, 53: 12, 54: 8, 55: 8, 56: 9, 57: 7, 58: 7, 59: 7, 60: 12, 61: 12, 62: 3, 63: 7, 64: 9, 65: 10, 66: 10, 67: 7, 68: 14, 69: 5, 70: 7, 71: 8, 72: 9, 73: 10, 74: 5, 75: 4, 76: 8, 77: 2, 78: 3, 79: 6, 80: 6, 81: 7, 82: 10, 83: 5, 84: 1, 85: 5, 86: 10, 87: 4, 88: 6, 89: 7, 90: 11, 91: 3, 92: 3, 93: 7, 94: 7, 95: 3, 96: 4, 97: 4, 98: 6, 99: 1, 100: 3, 101: 4, 102: 4, 103: 7, 104: 5, 105: 4, 106: 4, 107: 6, 108: 5, 109: 1, 110: 4, 111: 5, 112: 4, 113: 2, 114: 3, 115: 2, 116: 3, 117: 2, 118: 4, 119: 3, 120: 5, 121: 4, 122: 6, 123: 3, 124: 2, 125: 3, 126: 3, 127: 2, 128: 6, 129: 4, 130: 4, 131: 3, 132: 3, 133: 2, 134: 1, 137: 3, 138: 1, 139: 1, 652: 1, 141: 1, 142: 2, 143: 1, 145: 1, 146: 2, 147: 4, 150: 4, 151: 1, 152: 1, 153: 1, 154: 2, 155: 2, 156: 3, 157: 1, 158: 1, 159: 1, 160: 3, 673: 1, 162: 1, 163: 1, 164: 3, 165: 3, 167: 2, 168: 1, 169: 1, 170: 2, 171: 3, 173: 1, 174: 1, 689: 1, 180: 1, 182: 1, 183: 1, 186: 2, 187: 1, 190: 1, 196: 1, 197: 1, 199: 2, 201: 3, 202: 1, 206: 1, 207: 1, 221: 1, 223: 1, 229: 1, 230: 1, 241: 2, 252: 1, 766: 1, 261: 1, 284: 1, 291: 1, 303: 1, 1846: 1, 311: 1, 315: 1, 320: 1, 343: 1, 536: 1, 360: 1, 363: 1, 409: 1, 161: 1, 177: 1}\n",
    "\n",
    "    make_hist(dictionary, \"numWordsDST\")\n",
    "\n",
    "# Running the analysis \n",
    "main2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[hist_lenPLAIN_DST.png](hist_lenPLAIN_DST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "db_file = 'data/kaggle_norm_competition/en_train.csv'                    # Here you should put the path to the file you want to change\n",
    "\n",
    "outputFile = open (\"data/outputTrainDST\", 'a')\n",
    "with open (db_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    counter = 0\n",
    "    pastPhrase = 0\n",
    "    reader.__next__()\n",
    "    phraseString = ''\n",
    "    for row in reader:\n",
    "        if pastPhrase == int (row[0]):\n",
    "            #We are still on the current phrase\n",
    "            phraseString += row[4] + ' '\n",
    "        else:\n",
    "            print (phraseString, file = outputFile)\n",
    "            phraseString = \"\"\n",
    "            phraseString += row[4] + ' '\n",
    "\n",
    "            counter += 1\n",
    "            if counter == 20000:\n",
    "                break\n",
    "\n",
    "        pastPhrase = int (row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to run the tokenization of the database, some commands should be runned from shell terminal.\n",
    "\n",
    "``` {.bash}\n",
    "\n",
    "    cd attention-is-all-you-need-pytorch/\n",
    "\n",
    "    for l in en de; do for f in data/kaggle_norm_competition/*.$l; do if [[ \"$f\" != *\"test\"* ]]; then sed -i \"$ d\" $f; fi;  done; done\n",
    "    for l in en de; do for f in data/kaggle_norm_competition/*.$l; do perl tokenizer.perl -a -no-escape -l $l -q  < $f > $f.atok; done; done\n",
    "\n",
    "    python preprocess.py -train_src data/kaggle_norm_competition/linesTrainSRC -train_tgt data/kaggle_norm_competition/linesTrainDST -valid_src data/kaggle_norm_competition/linesValSRC -valid_tgt data/kaggle_norm_competition/linesValDST -save_data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt\n",
    "\n",
    "    cd ..\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {.bash}\n",
    "\n",
    "python attention-is-all-you-need-pytorch/train.py -data data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -save_model trained -save_mode best -proj_share_weight\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {.bash}\n",
    "\n",
    "python attention-is-all-you-need-pytorch/translate.py -model trained.chkpt -vocab data/kaggle_norm_competition/train_kaggle2transformer.atok.low.pt -src data/kaggle_norm_competition/linesTest\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The code\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim                                                                                                                                                                                                                   \n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'attention-is-all-you-need-pytorch')\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "from transformer.Modules import BottleLinear as Linear\n",
    "from transformer.Layers import EncoderLayer, DecoderLayer                                                                                                                                                                                                     \n",
    "from transformer.Models import Transformer                                                                                                                                                                                                    \n",
    "from transformer.Optim import ScheduledOptim                                                                                                                                                                                                  \n",
    "from DataLoader import DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word classification trial\n",
    "\n",
    "In order to take into account the tags from the database (the word class), we add an extra decoder in the Transformer that only output the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):                                                                                                                                                                                                               \n",
    "                                                                                                                                                                   \n",
    "\n",
    "    def __init__(                                                                                                                                                                                                                             \n",
    "            self, n_src_vocab, n_tgt_vocab, n_cls_vocab, n_max_seq, n_layers=6, n_head=8,                                                                                                                                                     \n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,                                                                                                                                                                    \n",
    "            dropout=0.1, proj_share_weight=True, embs_share_weight=True):                                                                                                                                                                     \n",
    "\n",
    "        super(MyTransformer, self).__init__()                                                                                                                                                                                                 \n",
    "        self.encoder = Encoder(                                                                                                                                                                                                               \n",
    "            n_src_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         \n",
    "            d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           \n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         \n",
    "        self.decoder = Decoder(                                                                                                                                                                                                               \n",
    "            n_tgt_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         \n",
    "            d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           \n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         \n",
    "        self.tgt_word_proj = Linear(d_model, n_tgt_vocab, bias=False)                                                                                                                                                                         \n",
    "        self.dropout = nn.Dropout(dropout)                                                                                                                                                                                                    \n",
    "\n",
    "        self.decoder_cls = Decoder(                                                                                                                                                                                                           \n",
    "            n_cls_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,                                                                                                                                                                         \n",
    "            d_word_vec=d_word_vec, d_model=d_model,                                                                                                                                                                                           \n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)                                                                                                                                                                                         \n",
    "        self.cls_word_proj = Linear(d_model, n_cls_vocab, bias=False)                                                                                                                                                                         \n",
    "\n",
    "        assert d_model == d_word_vec, 'To facilitate the residual connections, the dimensions of all module output shall be the same.'                                                                                                                                                                              \n",
    "\n",
    "        if proj_share_weight:                                                                                                                                                                                                                 \n",
    "            # Share the weight matrix between tgt word embedding/projection                                                                                                                                                                   \n",
    "            assert d_model == d_word_vec                                                                                                                                                                                                      \n",
    "            self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight                                                                                                                                                                      \n",
    "\n",
    "        if embs_share_weight:                                                                                                                                                                                                                 \n",
    "            # Share the weight matrix between src/tgt word embeddings                                                                                                                                                                         \n",
    "            # assume the src/tgt word vec size are the same                                                                                                                                                                                   \n",
    "            assert n_src_vocab == n_tgt_vocab, \"To share word embedding table, the vocabulary size of src/tgt shall be the same.\"\n",
    "            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight                                                                                                                                                               \n",
    "\n",
    "    def get_trainable_parameters(self):                                                                                                                                                                                                       \n",
    "        ''' Avoid updating the position encoding '''                                                                                                                                                                                          \n",
    "        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))                                                                                                                                                          \n",
    "        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))                                                                                                                                                          \n",
    "        dec_freezed_param_ids_cls = set(map(id, self.decoder_cls.position_enc.parameters()))                                                                                                                                                  \n",
    "\n",
    "        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids | dec_freezed_param_ids_cls                                                                                                                                         \n",
    "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)                                                                                                                                                               \n",
    "\n",
    "    def forward(self, src, tgt, cls):                                                                                                                                                                                                         \n",
    "        src_seq, src_pos = src                                                                                                                                                                                                                \n",
    "        tgt_seq, tgt_pos = tgt                                                                                                                                                                                                                \n",
    "        cls_seq, cls_pos = cls\n",
    "\n",
    "        #TODO: I don't understand what this is doing (only will modify the lenght of one sentence, the largest one)                                                                                                                           \n",
    "        tgt_seq = tgt_seq[:, :-1]                                                                                                                                                                                                             \n",
    "        tgt_pos = tgt_pos[:, :-1]                                                                                                                                                                                                             \n",
    "\n",
    "        cls_seq = cls_seq[:, :-1]                                                                                                                                                                                                             \n",
    "        cls_pos = cls_pos[:, :-1]                                                                                                                                                                                                             \n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_pos)                                                                                                                                                                                       \n",
    "\n",
    "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)                                                                                                                                                                  \n",
    "        seq_logit = self.tgt_word_proj(dec_output)                                                                                                                                                                                            \n",
    "\n",
    "        dec_cls_output, *_ = self.decoder_cls(cls_seq, cls_pos, src_seq, enc_output)                                                                                                                                                          \n",
    "        seq_cls_logit = self.tgt_word_proj(dec_cls_output)                                                                                                                                                                                    \n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2)), seq_cls_logit.view(-1, seq_cls_logit.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IN train.py\n",
    "\n",
    "def train_epoch(model, training_data, crit, optimizer):                                                                                                                                                                                       \n",
    "    ''' Epoch operation in training phase'''                                                                                                                                                                                                  \n",
    "                                                                                                                                                                                                                                              \n",
    "    model.train()                                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                                              \n",
    "    total_loss = 0                                                                                                                                                                                                                            \n",
    "    n_total_words = 0                                                                                                                                                                                                                         \n",
    "    n_total_correct = 0                                                                                                                                                                                                                       \n",
    "                                                                                                                                                                                                                                              \n",
    "    for batch in tqdm(                                                                                                                                                                                                                        \n",
    "            training_data, mininterval=2,                                                                                                                                                                                                     \n",
    "            desc='  - (Training)   ', leave=False):                                                                                                                                                                                           \n",
    "                                                                                                                                                                                                                                              \n",
    "        # prepare data                                                                                                                                                                                                                        \n",
    "        src, tgt, cls = batch                                                                                                                                                                                                                 \n",
    "                                                                                                                                                                                                                                              \n",
    "        gold = tgt[0][:, 1:]                                                                                                                                                                                                                  \n",
    "        class_gold = cls[0][:,1:]                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                                              \n",
    "        # forward                                                                                                                                                                                                                             \n",
    "        optimizer.zero_grad()                                                                                                                                                                                                                 \n",
    "        pred = model(src, tgt)                                                                                                                                                                                                                \n",
    "                                                                                                                                                                                                                                              \n",
    "        # backward                                                                                                                                                                                                                            \n",
    "        loss, n_correct = get_performance(crit, pred, gold)                                                                                                                                                                                   \n",
    "        loss.backward()                                                                                                                                                                                                                       \n",
    "                                                                                                                                                                                                                                              \n",
    "        # update parameters                                                                                                                                                                                                                   \n",
    "        optimizer.step()                                                                                                                                                                                                                      \n",
    "        optimizer.update_learning_rate()                                                                                                                                                                                                      \n",
    "                                                                                                                                                                                                                                              \n",
    "        # note keeping                                                                                                                                                                                                                        \n",
    "        n_words = gold.data.ne(Constants.PAD).sum()                                                                                                                                                                                           \n",
    "        n_total_words += n_words                                                                                                                                                                                                              \n",
    "        n_total_correct += n_correct                                                                                                                                                                                                          \n",
    "        total_loss += loss.data[0]                                                                                                                                                                                                            \n",
    "                                                                                                                                                                                                                                              \n",
    "    return total_loss/n_total_words, n_total_correct/n_total_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
